MODEL_NAME = "meta-llama/Llama-3.2-3B" # Pre-trained model
DATASET_NAME = "ola13/small-the_pile" # Path of dataset for training and evaluation
TRAIN_BATCH_SIZE = 16 # Number of samples processed before the model's internal parameters are updated during training
EVAL_BATCH_SIZE = 16 # Number of samples processed during evaluation to calculate metrics
GRADIENT_ACCUMULATION_STEPS = 8 # Number of steps for which gradients are accumulated before performing a backward/update pass
PRETRAINED_LEARNING_RATE = 1e-5 # Step size at each iteration while moving toward a minimum of the loss function for the base model
PALLM_LEARNING_RATE = 4e-3 # For the custom components
NUM_TRAIN_EPOCHS = 4 # Number of times the entire dataset is passed through the model during training
WARMUP_STEPS = 100 # Number of steps for gradually increasing the learning rate from 0 to the set value
MAX_SEQ_LENGTH = 1024 # The maximum sequence length for input data; longer sequences will be truncated
TRAIN_RATIO = 0.8 # The ratio of the dataset to be used for training; the remainder is used for validation/testing
