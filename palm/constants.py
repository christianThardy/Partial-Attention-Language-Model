MODEL_NAME = "meta-llama/Llama-3.2-3B"
DATASET_NAME = "ola13/small-the_pile"
TRAIN_BATCH_SIZE = 16 # Number of samples processed before the model's internal parameters are updated during training
EVAL_BATCH_SIZE = 16 # Number of samples processed during evaluation to calculate metrics
GRADIENT_ACCUMULATION_STEPS = 8 # Number of steps for which gradients are accumulated before performing a backward/update pass
PRETRAINED_LEARNING_RATE = 4e-5 # Step size at each iteration while moving toward a minimum of the loss function for the base model
PALLM_LEARNING_RATE = 6.2e-5 # Same, but for the new mechanisms
NUM_TRAIN_EPOCHS = 4 
WARMUP_STEPS = 500 # Number of steps for gradually increasing the learning rate from 0 to the set value
MAX_SEQ_LENGTH = 1024 # The maximum sequence length for input data; longer sequences will be truncated
TRAIN_RATIO = 0.8
LOG_EVERY_N_STEPS = 1
USE_FREEZE_IN_CHUNKS = False # Freeze/unfreeze layers chunk-by-chunk over certain steps
USE_DYNAMIC_SAE_WEIGHT = False # Vary the SAE weight across epochs
USE_ALL_LAYERS_SMALL_LR = # Train all layers from the start with smaller LR for base
USE_COSINE_DECAY = True # Learning rate schedule
USE_POLYNOMIAL_DECAY = False # Learning rate schedule
USE_LORA = False # Apply LoRA adapters
USE_QLORA = False # Apply QLoRA (AdaLoRA) adapters
